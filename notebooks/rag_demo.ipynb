{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062808a8",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3617a",
   "metadata": {},
   "source": [
    "## Suppress warnings from MuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SuppressMuPDFWarnings:\n",
    "    def __enter__(self):\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197cbf9",
   "metadata": {},
   "source": [
    "## Define the RAGPipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, pdf_folder=PROJECT_ROOT / \"data\", embedding_model_path=PROJECT_ROOT / \"models\" / \"paraphrase-MiniLM-L6-v2\",\n",
    "                 llm_model_path=PROJECT_ROOT / \"models\" / \"falcon-rw-1b\", top_k=3):\n",
    "        self.pdf_folder = pdf_folder\n",
    "        self.top_k = top_k\n",
    "        self.embedder = SentenceTransformer(str(embedding_model_path), local_files_only=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(str(llm_model_path), local_files_only=True)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(str(llm_model_path), local_files_only=True)\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.generator = pipeline(\"text-generation\", model=self.llm_model, tokenizer=self.tokenizer, device=device)\n",
    "\n",
    "    def load_pdfs(self):\n",
    "        documents = []\n",
    "        for pdf_path in Path(self.pdf_folder).glob(\"*.pdf\"):\n",
    "            try:\n",
    "                with SuppressMuPDFWarnings():\n",
    "                    with fitz.open(pdf_path) as doc:\n",
    "                        text = \"\".join([page.get_text() for page in doc])\n",
    "                        if len(text.strip()) > 10:\n",
    "                            documents.append({\"filename\": pdf_path.name, \"content\": text})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {pdf_path.name}: {e}\")\n",
    "        return documents\n",
    "\n",
    "    def chunk_text(self, text, max_tokens=200):\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        chunks, current_chunk = [], \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk.split()) + len(sentence.split()) < max_tokens:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return chunks\n",
    "\n",
    "    def embed_chunks(self, documents):\n",
    "        embedded_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.chunk_text(doc[\"content\"])\n",
    "            for chunk in chunks:\n",
    "                embedding = self.embedder.encode(chunk)\n",
    "                embedded_chunks.append({\n",
    "                    \"filename\": doc[\"filename\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding\n",
    "                })\n",
    "        return embedded_chunks\n",
    "\n",
    "    def retrieve_relevant_chunks(self, question, embedded_chunks):\n",
    "        question_embedding = self.embedder.encode(question)\n",
    "        similarities = [\n",
    "            (cosine_similarity([question_embedding], [chunk[\"embedding\"]])[0][0], chunk)\n",
    "            for chunk in embedded_chunks\n",
    "        ]\n",
    "        sorted_chunks = sorted(similarities, key=lambda x: x[0], reverse=True)\n",
    "        return [chunk for _, chunk in sorted_chunks[:self.top_k]]\n",
    "\n",
    "    def generate_answer(self, question, retrieved_chunks):\n",
    "        context = \"\\n\".join([chunk[\"text\"] for chunk in retrieved_chunks])\n",
    "        prompt = (\n",
    "            f\"Use the following information to answer the instruction briefly and precisely.\\n\"\n",
    "            f\"Context:\\n{context}\\n\\nInstruction: {question}\\nAnswer:\"\n",
    "        )\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        answer_start = response.find(\"Answer:\")\n",
    "        answer = response[answer_start + len(\"Answer:\"):].strip() if answer_start != -1 else response.strip()\n",
    "\n",
    "        lines = answer.splitlines()\n",
    "        seen, unique_lines = set(), []\n",
    "        for line in lines:\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and clean_line not in seen:\n",
    "                unique_lines.append(clean_line)\n",
    "                seen.add(clean_line)\n",
    "\n",
    "        return \" \".join(unique_lines)\n",
    "\n",
    "    def evaluate_answer(self, question, answer):\n",
    "        question_embedding = self.embedder.encode(question)\n",
    "        answer_embedding = self.embedder.encode(answer)\n",
    "        similarity = cosine_similarity([question_embedding], [answer_embedding])[0][0]\n",
    "        return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63a6d2",
   "metadata": {},
   "source": [
    "## Initialize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef785893",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057f493",
   "metadata": {},
   "source": [
    "## Load PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e074ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = rag.load_pdfs()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7804d",
   "metadata": {},
   "source": [
    "## Embed chunks from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10be144",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks = rag.embed_chunks(documents)\n",
    "print(f\"Embedded {len(embedded_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf8358",
   "metadata": {},
   "source": [
    "## Ask a question and retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931eb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does the API layer keep training synchronized across Swarm nodes?\"\n",
    "relevant_chunks = rag.retrieve_relevant_chunks(question, embedded_chunks)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\\n{chunk['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637913e",
   "metadata": {},
   "source": [
    "## Generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag.generate_answer(question, relevant_chunks)\n",
    "print(f\"\\nAnswer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37083b1",
   "metadata": {},
   "source": [
    "## Evaluate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = rag.evaluate_answer(question, answer)\n",
    "print(f\"\\nSimilarity Score: {similarity_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
