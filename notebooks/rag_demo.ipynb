{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062808a8",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3617a",
   "metadata": {},
   "source": [
    "## Suppress warnings from MuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a58fd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SuppressMuPDFWarnings:\n",
    "    def __enter__(self):\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197cbf9",
   "metadata": {},
   "source": [
    "## Define the RAGPipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b15d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, pdf_folder=PROJECT_ROOT / \"data\", embedding_model_path=PROJECT_ROOT / \"models\" / \"paraphrase-MiniLM-L6-v2\",\n",
    "                 llm_model_path=PROJECT_ROOT / \"models\" / \"falcon-rw-1b\", top_k=3):\n",
    "        self.pdf_folder = pdf_folder\n",
    "        self.top_k = top_k\n",
    "        self.embedder = SentenceTransformer(str(embedding_model_path), local_files_only=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(str(llm_model_path), local_files_only=True)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(str(llm_model_path), local_files_only=True)\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.generator = pipeline(\"text-generation\", model=self.llm_model, tokenizer=self.tokenizer, device=device)\n",
    "\n",
    "    def load_pdfs(self):\n",
    "        documents = []\n",
    "        for pdf_path in Path(self.pdf_folder).glob(\"*.pdf\"):\n",
    "            try:\n",
    "                with SuppressMuPDFWarnings():\n",
    "                    with fitz.open(pdf_path) as doc:\n",
    "                        text = \"\".join([page.get_text() for page in doc])\n",
    "                        if len(text.strip()) > 10:\n",
    "                            documents.append({\"filename\": pdf_path.name, \"content\": text})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {pdf_path.name}: {e}\")\n",
    "        return documents\n",
    "\n",
    "    def chunk_text(self, text, max_tokens=200):\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        chunks, current_chunk = [], \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk.split()) + len(sentence.split()) < max_tokens:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return chunks\n",
    "\n",
    "    def embed_chunks(self, documents):\n",
    "        embedded_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.chunk_text(doc[\"content\"])\n",
    "            for chunk in chunks:\n",
    "                embedding = self.embedder.encode(chunk)\n",
    "                embedded_chunks.append({\n",
    "                    \"filename\": doc[\"filename\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding\n",
    "                })\n",
    "        return embedded_chunks\n",
    "\n",
    "    def retrieve_relevant_chunks(self, question, embedded_chunks):\n",
    "        question_embedding = self.embedder.encode(question)\n",
    "        similarities = [\n",
    "            (cosine_similarity([question_embedding], [chunk[\"embedding\"]])[0][0], chunk)\n",
    "            for chunk in embedded_chunks\n",
    "        ]\n",
    "        sorted_chunks = sorted(similarities, key=lambda x: x[0], reverse=True)\n",
    "        return [chunk for _, chunk in sorted_chunks[:self.top_k]]\n",
    "\n",
    "    def generate_answer(self, question, retrieved_chunks):\n",
    "        context = \"\\n\".join([chunk[\"text\"] for chunk in retrieved_chunks])\n",
    "        prompt = (\n",
    "            f\"Use the following information to answer the instruction briefly and precisely.\\n\"\n",
    "            f\"Context:\\n{context}\\n\\nInstruction: {question}\\nAnswer:\"\n",
    "        )\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        answer_start = response.find(\"Answer:\")\n",
    "        answer = response[answer_start + len(\"Answer:\"):].strip() if answer_start != -1 else response.strip()\n",
    "\n",
    "        lines = answer.splitlines()\n",
    "        seen, unique_lines = set(), []\n",
    "        for line in lines:\n",
    "            clean_line = line.strip()\n",
    "            if clean_line and clean_line not in seen:\n",
    "                unique_lines.append(clean_line)\n",
    "                seen.add(clean_line)\n",
    "\n",
    "        return \" \".join(unique_lines)\n",
    "\n",
    "    def evaluate_answer(self, question, answer):\n",
    "        question_embedding = self.embedder.encode(question)\n",
    "        answer_embedding = self.embedder.encode(answer)\n",
    "        similarity = cosine_similarity([question_embedding], [answer_embedding])[0][0]\n",
    "        return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63a6d2",
   "metadata": {},
   "source": [
    "## Initialize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef785893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "rag = RAGPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057f493",
   "metadata": {},
   "source": [
    "## Load PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e074ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n"
     ]
    }
   ],
   "source": [
    "documents = rag.load_pdfs()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7804d",
   "metadata": {},
   "source": [
    "## Embed chunks from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d10be144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 142 chunks.\n"
     ]
    }
   ],
   "source": [
    "embedded_chunks = rag.embed_chunks(documents)\n",
    "print(f\"Embedded {len(embedded_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf8358",
   "metadata": {},
   "source": [
    "## Ask a question and retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "931eb78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Framework architecture \n",
      "API layer \n",
      "Swarm Learning is implemented as an API library available for multiple popular frameworks such as TensorFlow, Keras, and such. These  \n",
      "APIs provide an interface that is similar to the training APIs in the native frameworks familiar to the data scientists. Calling these APIs \n",
      "automatically inserts the required hooks for Swarm Learning so that nodes seamlessly exchange parameters at the end of each model \n",
      "training epoch, and subsequently continue the training aft...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "The boot-up is an ordered process in which the set of participant nodes \n",
      "designated as peer-discovery nodes (during the initialization phase) are booted up first, followed by the rest of the nodes in the network. \n",
      "Technical white paper \n",
      "Page 7 \n",
      " \n",
      "Integration and training \n",
      "Swarm Learning provides a set of simple APIs to enable swift integration with multiple frameworks. These APIs are incorporated into the \n",
      "existing code base to quickly transform a stand-alone ML node into a Swarm Learning partic...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Moreover, the merge process is not done by a static central \n",
      "coordinator or parameter server, rather a temporary leader chosen dynamically among the nodes is used to perform the merge, thereby \n",
      "making the Swarm network decentralized. This provides a far greater fault-tolerance than traditional centralized-parameter-server-based \n",
      "frameworks. With the global model, the nodes have the collective intelligence of the network at their disposal, without the data ever leaving \n",
      "the node. \n",
      "Swarm Learning ...\n"
     ]
    }
   ],
   "source": [
    "question = \"How does the API layer keep training synchronized across Swarm nodes?\"\n",
    "relevant_chunks = rag.retrieve_relevant_chunks(question, embedded_chunks)\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\\n{chunk['text'][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637913e",
   "metadata": {},
   "source": [
    "## Generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a5b1603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The API layer keeps training synchronized across Swarm nodes by using the following two mechanisms: 1. The API layer provides a set of APIs that are used to initialize the Swarm network and to train the local model. These APIs are implemented in the native ML frameworks, such as TensorFlow, Keras, and so on. These APIs are used to initialize the Swarm network and to train the local model. 2. The API layer also provides a set of APIs that are\n"
     ]
    }
   ],
   "source": [
    "answer = rag.generate_answer(question, relevant_chunks)\n",
    "print(f\"\\nAnswer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37083b1",
   "metadata": {},
   "source": [
    "## Evaluate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588f8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "similarity_score = rag.evaluate_answer(question, answer)\n",
    "print(f\"\\nSimilarity Score: {similarity_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
